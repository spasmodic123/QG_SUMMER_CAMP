# pytprch搭建神经网络一般方法

1. 扩展nn.Module基类，

   创建一个**继承**nn.Module基类的神经网络类

2. 定义层为类属性，

   ​	使用torch.nn中预建的层定义网络的层作为类的属性

3. 实现forward()方法

   使用network的层属性以及nn.functional API中的操作定义网络的前向传递





<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689232456617.png"/>

可以用魔方的形状类比**滤波器(filter)**的形状,kernel_size是滤波器的尺寸(魔方的长和宽),in_channel是滤波器的深度(魔方的高),out_channel是输出的数量,也就是滤波器的数量(魔方的个数)



# RNN

RNN(循环神经网络)是一种善于对序列数据建模的神经网络.

## 何为序列

我们拍了一堆关于球的静态快照,这是其中一张,如果我们想只通过这一张照片预测球的下一步运动是做不到的

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230713204957.png"/>

而如果我们记录了一系列球的静态快照,我们就可以预测球的下一步运动了

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1_2UsTgXbxwHXYmFmskHL-9w.gif"/>这就是序列,又称为顺序,一个事物跟着另一个事物的顺序

CNN擅长处理序列数据

## 与普通神经网络的不同

普通神经网络(前馈神经网络)

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230713212202.png"/>

循环神经网络

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1_T_ECcHZWpjn0Ki4_4BEzow.gif"/>

多了个循环,传递隐藏状态到下一个时间步骤,隐藏状态代表之前的输入

## 例子

语音输入一句话:What time is it?

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/111.gif"/>

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1_3bKRTcqSbto3CXfwshVwmQ.gif"/>

## 梯度消失问题

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230713213132.png"/>

观察奇怪的颜色分布,越往前的数据,越往后传输,占比越小

短期记忆和梯度消失是由于反向传播的性质造成的；反向传播是一种用于训练和优化神经网络的算法。为了理解为什么会出现这种情况，让我们来看看反向传播对深度前馈神经网络的影响。

训练神经网络有三个主要步骤。首先，进行前向传递并做出预测。其次，使用损失函数将预测结果与实际情况进行比较。损失函数会输出一个误差值，该误差值是对网络性能有多差的估计。最后，利用误差值进行反向传播，计算网络中每个节点的梯度。

梯度越大，调整幅度越大，反之亦然。问题就出在这里。在进行反向传播时，层中的每个节点都会根据前一层的梯度效果来计算自己的梯度。因此，如果之前图层的调整很小，那么当前图层的调整就会更小。

这导致梯度在向下反向传播时呈指数级缩小。由于梯度极小，内部权重几乎没有调整，因此前几层无法进行任何学习。这就是梯度消失问题。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1_8eriEDJZisidMG_yyEDEAA.gif"/>

## LSTM 和 GRU

为了解决梯度消失问题,有了两种专门的递归神经网络,一种是长短期记忆(Long Short-Term Memory)(LSTM),  另一种门递归单元(Gated Recurrent Units)(GRU)

## LSTM 和 RNN区别

所有递归神经网络都具有神经网络重复模块链的形式。在标准的RNN中，这种重复模块将具有非常简单的结构，例如一个单一的tanh层。<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689256655131.png"/>



LSTM也具有这种链状结构，但重复模块具有不同的结构。神经网络层不是一个，而是四个，它们以一种非常特殊的方式相互作用。<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689256672285.png"/>

LSTM背后的核心理念 LSTM的关键在于单元态，即贯穿图表顶部的水平线。

单元状态有点像传送带。它沿着整个链条笔直向下运行，只有一些微小的线性交互。信息很容易沿着它一成不变地流动。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689257662133.png"/>