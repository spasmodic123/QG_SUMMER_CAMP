# RNN(循环神经网络  递归神经网络)

## RNN的特点

相比神经网络,多了一个反馈环

 ![image-20230717165146152](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717165146152.png)

这个反馈环如何使用,举一个预测股票的例子

![1689583981926](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689583981926.png)

比如,昨天的股票价格是0,今天的股票价格是0,我们想预测明天的股票



![image-20230717165441972](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717165441972.png)

先将昨天的数据0输入,0×1.8+0,经过激活函数=0,但这个时侯,我们**不是立刻继续前往输出层**

而是**经过反馈环(乘以一个权重或者权重矩阵)**,和今天的数据结合,**两天的数据共同预测**明天的输出,

relu( relu(0×1.8+0)x -0.5 + 0×1.8+0 )=0,这时候才输出0

反馈换我们也可以用递归结构表示,相当于将神经网络复制了一份,复制的网络==**权重,偏置量保持不变,只是输入不同**==,LSTM一样道理

![image-20230717170020287](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717170020287.png)

同理,如果我们我们知道了3天的数据想预测第4天的数据,相当于反馈环循环三次,或者网络复制3次

# LSTM

![1689587385624](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689587385624.png)

RNN无法解决长期记忆,梯度爆炸或梯度消失的问题,

LSTM与RNN不同之处在于,相比于RNN的只有一个反馈环,LSTM分开两个路径,一个负责长期记忆吗,一个负责短期记忆

LSTM使用的激活函数为sigmoid函数和tanh函数

## 具体过程

![1689590345265](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689590345265.png)

上面绿色的线叫做**cell state**,负责传输**长期记忆**,可以看到,它会经过乘法和加法做出调整,但不是经过权重和偏置量直接调整,这也就避免了梯度消失和梯度爆炸的问题
下面紫色的线,称为**hidden state**,负责传输**短期记忆**,可以之间通过权重和偏置量进行调整

下面我们介绍具体运算

![1689590946203](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689590946203.png)

![1689590824009](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689590824009.png)

我们可以看到,LSTM的第一阶段的作用更像是告诉神经网络,**长期记忆应该记住百分之多少**的长期记忆,我们看上面的两个例子,如果输入1,长期记忆的值2将乘上0.997,几乎不变,全部记住;如果输入时-10,那么长期记忆将乘0,全部忘记.(我们称**蓝色**这一阶段为--==遗忘门==)

![image-20230717185551049](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717185551049.png)

黄色的格子创造**可能存在的长期记忆**

而绿色的格子决定了**百分之多少的可能存在的长期记忆,应该被添加到真正的长期记忆**中

为何两个格子的作用不同,跟他们的激活函数有关,我举例子说明,假设input是1,那么黄色格子的输出值为0.97**(代表存在0.97的可能长期记忆)**,绿色格子的输出值几乎为1.0**(代表应该把全部的可能长期记忆加到真正的长期记忆中去)**,*为什么呢,因为短期记忆为1,输入又是1,好像两者存在规律,这是长期记忆应该记住的,所以值为1.0和0.97这么大*

相反,如果input是-10,那么黄色的输出值为-1 **(代表存在-1的可能长期记忆)**,绿色格子的输出值几乎为0**(不应该将这个输入添加到长期记忆中去)**,

因为短期记忆是1,输入时-10,两者差别太大,不像是一种规律

总之,绿色格子和黄色格子称为一个阶段,这个阶段决定我们应该**更新多少的长期记忆**,(这个**绿色**阶段称为--==输入门==)

![image-20230717191409138](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717191409138.png)

这一个阶段负责对短期记忆进行修改,有紫色格子和红色格子,红色格子代表**可能存在的短期记忆**,紫色格子代表**百分之多少的短期记忆,应该被添加到真正的短期记忆**中.(这个**紫色**阶段称为--==输出门==)

整个过程如下,2.96作为最终长期记忆,0.98作为最终短期记忆继续往下传播,处理下一个输入

![image-20230717191816657](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717191816657.png)

现在,有一个场景,知道了股票的前几天的情况(多个输入),预测下一天的情况,LSTM的全过程如下

![image-20230717192018294](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230717192018294.png)

==上面有4个LSTM单元,他们的权重和bias一致==

## LSTM参数讲解

![image-20230718154136428](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718154136428.png)

input_size为输入的数据的维度(特征),还是以上面预测股票的例子,每次输入(每一天的输入)只有一个数值,所谓input_size=1,而如果每次输入(每一天的输入)不仅有当天的一个数值,还有交易的数量,还有交易人的经济水平,那input_size=3.        图中几个绿色圆,输入维度就是几,也就输数据有几个特征

hidden_size为从LSTM层输出的维度,也就是输入到全连接层的维度.

num_layers就是为**隐藏层的层数,**官方建议1层或2层.一个隐藏层包括多个**递归结构**.

不仅如此,对于数据的输入,也要有对应的结构

x:[seq_length, batch_size, input_size]

![image-20230718155209653](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718155209653.png)

input_size就是数据有几个特征

seq_len为序列长度,也就是时间序列有多长,还是以上面预测股票作为例子,如果有8天的数据,那么seq_len=8,有10天的数据,则seq_len=10

batch_size肯定就是批次的大小