

# Tensorboard

```python
from torch.utils.tensorboard import SummaryWriter
```

**特定的数据结构才能输入到SummaryWriter中,所以必须将图片对象转化形式,转化成tensor或numpy格式**

## add_add_histogram

作用:绘制直方图,查看**分布范围**

参数:(name,value,step)   ,对应绘制图像的名字,图像的分布数值(比如说一个列表),图像的步数,观察每走一步数值的分布可视化变化

例子:

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230714102102.png"/>

我们看向第一个数据块，它代表writer.add_histogram('distribution centers', x + i, 0)的调用，此时global_step=0。相同global_step的数据会被放置在同一个层。

上图指向的块高度为336，横坐标为0.167，纵坐标为0，这代表第0层数据在0.167附近的点有336个。<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230714102139.png"/>
<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230714102132.png"/>

我们把这一层的另外两个点分布截图，高度分别为324和340。336+324+340=1000，这就与x = np.random.random(1000)对应上了。直方图显示的就是这1000个点的数值分布情况。


## add_scalar

作用:绘制数值数值**变化**图像

参数:(name,value,step)   名字,值,步数,查看每一步数值的变化

例子:

```python
from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
```

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689301760546.png"/>

## add_graph

作用:绘制模型

参数:(*model*, *input_to_model*)   需要绘画的模型,  训练数据

例子:卷积神经网络模型

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689303414183.png"/><img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689303473337.png"/>





# 反向传播

## **==链式法则贯穿损失函数的梯度计算==**

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334224791.png"/>

**损失函数实际上是许多函数的嵌套,在求导求梯度时运用链式法则**

单个结点的损失函数可以表达为:(预测值-真实值)的平方<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334423478.png"/>

所以,总的损失函数可表示为:<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334429161.png"/>

对应结点的输入,其实是连接该节点的所有节点的权重和:<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334545507.png"/>

最终输出结点的输出(即预测输出),是结点的输入,再经过激活函数的输出,激活函数为g(),所以:<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334758735.png"/>

的而因为,当个结点的损失为预测值减去真实值的差的平方,真实值是已知量,可以当作常数,所以单个结点的损失可以看成是关于预测输出的函数:<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689334819661.png"/>

而因为单个结点的输入z又可以看做是权重的函数,所以,损失函数最终可以表达为:<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689335223305.png"/>

最终其实是**权重的函数**

然后我们来计算梯度,我们简单起见,只用一条边,只计算一个权重的梯度,其他的类比.求**该层的结点1**与**上一层的结点2**的梯度,     根据上面单个结点损失函数表达式,又**j=1,k=2**,我们对该权重求导:

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689336668508.png"/>

又因为上式表达的是单个样本输入,也就是一个数据的损失函数梯度,假设有n个数据,**该个权重的梯度**表示为所有数据的梯度平均值:

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689336840623.png"/>

其他每一个在**输出层**的权重类似

---

---

上面说了那么多,其实那里只计算了输出层权重的梯度,那么其他层呢?

我们明显注意到,其他层的计算与输出层的计算是不同的,但是,我们可以类比啊,

比如,我们想求出倒数第二层(隐藏层)的结点2与倒数第三层(隐藏层)的结点2之间的权重梯度,根据上面的公式,显然:

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689340985577.png"/>

很简单是吧,但是我们细看,我们可能发现问题,损失函数C对于倒数第二层a2的导数,损失函数C没有直接作用于a2,a2是在隐藏层,而损失函数作用于输出层

但是,我们发现,损失函数直接作用于输出层的激活函数a1,间接作用于隐藏层激活函数a2,竟然有这种关系!

**链式法则**登场

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689341462876.png"/>

然后我们把这一个式子带入到上面那个对ω22求导的式子,就可以得出结果

就相当于,无论泥鳅任意一个**隐藏层**的权重,都要从输出层的权重往前推,而往前推的过程,就需要**链式法则**将每一层的激活函数链起来,一层一层的往前面传播

整个过程称为:==反向传播!==



# 梯度消失

梯度消失是训练神经网络经常出现的一个大问题,而且经常出现在**比较靠前的层**

有的时候,在靠前的层的梯度变得非常小,我们称为梯度消失

我们学过,权重的更新是:权重-学习率*梯度,  如果梯度太小,造成的后果就是几乎没有更新,损失函数几乎无法缩小.

## 原因

我们从所学的反向传播知识中得知，任何给定权重的损失梯度都将是一些导数的乘积，而这些导数取决于网络中后层的权重。根据链式法则,从后往前一层一层的链着.

根据**链式法则**，我们可以推断出，权重在网络中的位置越靠前，我们刚才提到的乘积中需要的项就越多，从而得到相对于该权重的损失梯度。

现在的关键是要理解，如果这个乘积中的项，或者至少其中的一些项很小，会发生什么情况？所谓小，是指小于1。那么，一堆小于1的数字的乘积会得到一个更小的数字

## 梯度爆炸

与梯度消失类似,位于后面层的几个梯度过大,也就是大于

## 为什么会梯度消失或者梯度爆炸

### 激活函数和随机初始化权重的问题,sigmoid函数作为激活函数

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230714233158.png"/>

根据链式法则,如果要求损失函数loss对于b1的梯度,算式如下<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689348771781.png"/>

我们离不开对激活函数的求导,而sigmod函数的导数图像如下

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689348980332.png"/>

我们看到梯度不会超过0.25,是小于1的,又因为随机初始化权重是初始化为均值为0,标准差为1的一堆数,也就是说,权重往往小于1,

这样的话,随着反向传播,越靠前面的层,根据上文的链式法则算式,一路累乘,得到的梯度越小

梯度爆炸也一般由权重随机初始化造成,如果随机初始化的值太大,根据链式法则一路往前面的层计算,一路累乘,就会梯度过大,导致前面的层的不仅越过了最优值,还会远离最优值

## 解决方法

Xavier Initialization (泽维尔初始化)

原本随机初始化的权重是均值为0,标准差为1的正态分布

Xavier Initialization就是初始化权重为均值为0,方差为1/n的分布,具体做法就是在随机初始化将权重设为均值为0,标准差为1的正态分布后,将每个权重乘以1/$\sqrt{n}$

---

但是,以上讲的是激活函数为sigmoid函数的情况下初始化权重为均值为0,方差为1/n的分布是比较优的;    如果是relu激活函数,那么最佳方差为2/n




