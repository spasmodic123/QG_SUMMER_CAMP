# Bias

## 概念

神经网络中的可学习参数之一

现在，从概念上讲，我们可以认为每个神经元的偏置具有类似于阈值的作用。这是因为偏置值将决定神经元的激活输出是否会在网络中向前传播。

换句话说，偏差决定神经元是否或以多大程度触发。它让我们知道神经元何时被有意义地激活。

增加这些偏倚最终会提高模型的灵活性，以适应给定的数据。

## bias在神经网络中的位置

由神经网络知识可以得知, 每个神经元会接收来自上一层的输入,也就输加权和,然后将加权和传递给激活函数.但不是直接将加权和传递给激活函数,而是**加上偏置项(bias)后再传递给激活函数**

## 作用

 提高数据的灵活性

举个简单的例子,我们的输入层只有两个结点,第一个结点value是1,第二个结点value为2 ,他们都会对隐藏层的第一个结点进行输出,我们随意的初始化权重给这两个到隐藏层的输入,-0.55和0.1

权重和为<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689385931043.png"/>

如果没有bias,直接传递给激活函数relu<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689385984311.png"/>

当激活输出为0时,该神经元被视为未激活,也就是不会启动,他的信息不会传递给下一个神经元,

而根据relu函数,大于等于0的数字等于本身,小于0的数字等于0.也就是说,这个神经元的**阈值**为0,要超过零才有意义

**从本质上讲，0是加权和决定神经元是否点燃的阈值**。

那么，如果我们想改变阈值呢？如果神经元的输入不为零，而是大于或等于 "-1"，那么神经元就应该点燃。

这就是偏差发挥作用的地方。

**bias是阈值的相反数**

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689386261362.png"/>

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689386274021.png"/>

现在，神经元被认为正在激活。

现在，模型在拟合数据时的灵活性有所提高，因为它可以在**更大的范围**内确定哪些值被认为是激活的，哪些值被认为是未激活的。

我们也可以反其道而行之，缩小我们认为被激活的神经元输出值的范围。例如，如果我们认为神经元的输出值大于或等于5时才应被视为激活，那么我们的偏差就是-5。



# 正则化

## 概念

正则化一种通过对复杂性进行惩罚来帮助减少过拟合,降低网络方差的一种工具.

复杂性:即使模型对于训练集有着很好的效果,但是无法泛化,对于测试机效果很差,网络复杂性质是其中一个原因.好比我们解决一道数学题,如果我们用了一个很复杂的方法,那么一般来说,这个复杂的方法就很难具有通用性

如果我们在模型中加入正则化，那么我们基本上是在牺牲模型对训练数据进行良好拟合的能力来换取模型对未曾见过的数据进行更好泛化的能力。

## 实现正则化

实现正则化实际是在我们损失函数中添加一个项,对于大权重进行惩罚

正则化后的损失函数<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230715150854.png"/>

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689404335210.png"/>

两条杠是一个二范数,n是网络中层的数量,ω是对应j层的权重矩阵, m是输入的数量,λ是正则化参数(也是一个超参数,需要寻找最优)

## 如何影响权重

因为损失函数的目的是降低损失函数的值,而观察我们增加的那一项,如果权重大,乘积便会变大,整个损失函数就变大,

加了这一项,就好比一个惩罚,如果权重设置太大,损失函数也会变得很大,而我们的梯度下降法,或者其他优化函数,就会激励模型去减小权重

# batch size

batch_size是指以一次性向机器输入的数据

如果batch_size大一点,会加快运算速率,因为我们的电脑可以进行并行运算.然而，这样做的代价是，即使我们的机器能够处理非常大的批次，模型的质量也可能会随着批次的增大而下降，最终可能导致模型无法很好地归纳其以前未见过的数据。

## batch_size不应该太大

Keskar 等人对小批量和大批量之间的性能差距提出了一种解释：使用小批量的训练倾向于收敛到**平坦的极小化**，该极小化在极小化的小邻域内仅略有变化，而大批量则收敛到**尖锐的极小化**，这变化很大。平面minimizers 倾向于更好地泛化，因为它们对训练集和测试集之间的变化更加鲁棒 。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689406812078.png"/>

此外，他们发现与大批量训练相比，小批量训练可以找到距离初始权重更远的最小值(也就是防止了局部最优)。他们解释说，小批量训练可能会为训练引入**足够的噪声**，以退出锐化minimizers 的损失池，而是找到可能更远的平坦minimizers 。

个人理解:好像我们通过看篮球比赛来学习篮球战术,我们看了10场比赛,其中A战术出现较多,我们不一定觉得A战术是最好的,因为我们才看了10场比赛不敢断定.    但是,如果我们看了100场比赛,A战术较多,我们可能就会断定A战术最优,因为看来那么多场比赛.但其实A战术并不是最优,只是在我们国家最优. 如果我们去到了其他国家看比赛,可能B战术才是最优的.

比赛数量(batch_size)看多了,反而给了我们错误的想法

# 迁移学习

迁移学习发生在,我们利用从解决一个问题中获得的知识,去解决另外一个**新的,但有关联**的问题

比如,我们有一个已经训练好的识别轿车的模型,但是我们如果用这个模型去识别卡车,肯定错误很高,但是,我们知道,卡车和轿车是有共同点的,比如都有车门,门把手,车灯,轮胎等等,这些就是关联性

我们不可能重新训练一个新的模型,因为这样太耗时耗力了.竟然轿车和卡车有关联性,我们就可以用训练好的轿车模型,去训练识别卡车这个任务,只是我们要进行微调

## 如何微调,fine-tune

回到我们刚才提到的例子，如果我们有一个已经训练好的识别汽车的模型，而我们希望对这个模型进行微调以识别卡车，我们可以首先导入我们在汽车问题上使用的原始模型。

为了简单起见，假设我们去掉了这个模型的最后一层。最后一层是对图像是否为汽车进行分类。移除这一层后，我们要重新添加一层，目的是对图像是否为卡车进行分类。

在某些问题中，我们可能希望**删除的不仅仅是最后一个图层**，我们可能希望**添加的不仅仅是一个图层**。这将**取决于每个模型的任务有多相似**。

模型末尾的图层可能已经学习了与原始任务相关的特定特征，而模型开始的图层通常会学习更多的通用特征，如边缘、形状和纹理。

修改好模型的结构之后,我们就要**冻结**这些来自原有模型的层

## 冻结隐藏层

就是来自**原来模型的层的权重保持不变**,不会随着训练的过程而改变,我们在训练过程中改变的权重只有我们添加的新的层的权重



# 批次归一化 (batch normalization)

归一化就是把数据放在同一个标尺上(同一个数值范围),常见的操作为将数据放到0到1之间

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689427903924.png"/>

## 为何归一化

每个样本的每个特征也可能存在很大差异。如果我们有一个特征对应的是一个人的年龄，而另一个特征对应的是这个人在过去五年中驾驶汽车的里程数，那么，我们同样可以看到，年龄和驾驶里程这两项**数据并不在同一范围内**。

这些非标准化数据集中的较大数据点可能会导致**神经网络的不稳定性**，因为相对较大的输入可能会通过网络中的各层向下串联，这可能会导致**梯度不平衡**，因此可能会导致著名的**梯度爆炸**问题。

## 进行batch归一化的位置和方法

**batch归一化用在各个层中的输出位置,还有刚开始的输入位置**

首先在输入之前归一化一次

然后,我们知道,数据输出前的最后一个步骤就是经过激活函数,batch_normalization做到第一步就是接收来自激活函数的输出,对其标准化,然后,在对激活函数的输出进行归一化处理后，batch_normalization将该归一化输出**乘以某个任意参数，然后将另一个任意参数与该乘积相加**。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689429593857.png"/>

注意,这里的**均值和标准差是一个向量,不是一个标量值**,向量

我们以CNN为例子,   因为我们不是传进去单张图片,而是将一个batch的图片传了进去,每一个图片都有自己对应的通道channel,每一个图片有自己的特征图

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689469142987.png"/>

feature1是第一张图片的特征图,featrue2是第二张图片的特征图,每个图片有两个通道,均值和标准差求的是**不同图片同一通道的均值和标准差**,有两个通道,所以向量有两个元素

g和b也是可学习参数

## 作用

归一化过程使得网络中的权重不会因数值过高或过低而失衡

在我们的模型中加入batch_normalization可以大大提高训练速度，并降低离群大权重对训练过程的过度影响。



# CNN变种

## ResNet

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689474920219.png"/>

## 解决什么问题

1. 梯度消失或梯度爆炸

2. 退化问题

   随着网络深度的不断增大，所引入的激活函数也越来越多，数据被映射到更加离散的空间，此时已经难以让数据回到原点

### 亮点

1. residual模块(残差结构)

   神经网络中增加线性转换分支

2. Batch Normallization

 ### 残差结构

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689432473506.png"/>

左边时层数较少的残差结构,右边是层数较多的残差结构

层数多采用另外一种残差结构因为可以节省参数

加号代表把主分支的,经过卷积之后的特征矩阵,加上侧分支的特征矩阵**(可以相加代表两个特征矩阵要具有相同的形状,高,宽,深度相等)**

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230716225530.png"/>

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689489702608.png"/>

实线残差结构和虚线残差结构的区别:

实现代表输出的特征矩阵和输入的特征矩阵形状一样,可以通过残差结构直接相加

虚线代表输出特征矩阵和输入的特征矩阵不一样,需要通过一个升维或降维的残差矩阵转换,才能相加

如何保证特征矩阵的形状,通过类似上图的运算

1. 输入图片的特征是56x56x256,高56宽56通道256
2. 经过第一层1x1的卷积核,步长stride为1,128个卷积核对应128个输出,图片特征变为了56x56x128
3. 第二层,3x3的卷积核,步长为2(导致维度缩减),128个卷积核对应128输出,图片特征变为了28x28x128
4. 同理,图片特征变为了28x28x512
5. 而右边的虚线部分,1x1的卷积核,步长为2,512个卷积核对应512个输出,图片特征变为28x28x512

右边1x1尺寸的卷积核的作用的升维和降维

建议把bn层,也就是batch normalization的位置放在卷积层和激活函数之间,但是有文章表面batch normalization放在激活函数之后,我个人理解,两者都可以     因为我们的最终目的是要将数据转化到0到1的范围内,满足均值0,方差1的分布,防止网络不平衡,而batch normalization放在激活函数前面和后面都能起到这个效果



