 dataset:提供一种方式去获取数据及其label

- 如何获得每一个数据及其label
- 告诉我们总共有多少数据

dataloader:为后面的网络提供不同的数据形式



# BP神经网络

**信号是正向传播的，而误差是反向传播的。**

举一个例子，某厂商生产一种产品，投放到市场之后得到了消费者的反馈，根据消费者的反馈，厂商对产品进一步升级，优化，一直循环往复，直到实现最终目的——生产出让消费者更满意的产品。产品投放就是“信号前向传播”，消费者的反馈就是“误差反向传播”。**这就是BP神经网络的核心**。 

## 激活函数

定义:在人工神经网络中，激活函数是将节点的输入映射到相应输出的函数。

激活函数进行某种类型的运算，将总和转换为一个数字，该数字通常介于某个下限和某个上限之间。这种变换通常是非线性变换。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689042383201.png"/>

**作用:激活函数是用来加入非线性因素的，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。**

### sigmoid函数

1. 对于大多数负输入，sigmoid 将把输入变换成一个非常接近于0的数字。 

2. 对于大多数正输入，sigmoid 将把输入变换成一个非常接近于1的数字。

3.  对于相对接近于0的输入，sigmoid 将把输入变换成一个介于0 和 1之间的数字。

   <img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689042990124.png"/>

   接近1,激活程度高;接近0,激活程度低

   

   ### relu函数

   ```python
   function relu(x) {
       if (x <= 0) {
           return 0;
       } else {
           return x;
       }
   }
   ```

   小于等于0则输出0,否则输出本身

   

## 反向传播

基本思想就是**通过计算输出层与期望值之间的误差来调整网络参数，从而使得误差变小**

### 误差公式

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1688984368600.png"/>

### 计算新的权重方法

1. 梯度下降
2. 随机梯度下降(当样本数据量很大的时候，这个梯度需要对所有样本都进行一次梯度计算并累加起来，这是一个非常高的计算开销,如果随机,就是每次仅抽样“部分”样本来计算梯度值（极端情况下仅抽取一个样本），当迭代次数足够多以后，模型最终也是可以“收敛”的)

3.adam:在梯度下降法基础上,通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率，加快收敛速率

### 权重反向更新

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1688984695758.png"/>

(l)为学习率

### 需要设计者自己设定的参数

学习率，隐含层的层数，每个隐含层的神经元个数，激活函数的选取，损失函数（代价函数）的选取等等，这些参数被称之为超参数。



# 神经网络基本模型keras搭建

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689041319239.png"/>

```python
from keras.models import Sequential
from keras.layers import Dense,Activation

layers = [
    Dense(units=6,input_shape=(8,), activation='relu'),  # units是输出的维度,input_shape代表输入层有两个节点,也就输输入维度是2,也就是两个特征,activation是激活函数
    Dense(units=6, activation='relu'),  # Dense是一种典型的层,称为全连接层或密集层,意思是该层的每一个节点连接上一层的所有节点和下一层的所有节点,深度学习还有很多其他层,卷积层就是另外一种
    Dense(units=4, activation='softmax'),
]
model1 = Sequential(layers)  # Sequential是一个顺序模型(序列模型),是线性层的序列堆栈,正如神经网络按层组织
model1.add(Dense(units=3,activation='relu'))
# 8个输入,6个输出到第二层
# 6个输入到第二层,6个输出给第三层
# 6个输入到第三层,4个输出到第四层
# 4个输入到第四层,3个输入到第五层
```

