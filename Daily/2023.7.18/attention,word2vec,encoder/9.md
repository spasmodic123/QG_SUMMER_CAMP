# Attention

![1689675151588](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689675151588.jpg)

**Attention 机制听起来高大上，其关键就是学出一个权重分布，然后作用在特征上**

这个概念其实比较复杂,先从简单的讲起,一些基础内容,然后再次绕回来讲attention

## word2vec

将'词语'转化为'数字',才能参与机器运算

但是数字不是随机的分配个'词语',世界上那么多词语,有的是近义词,同一个词语也可以用到不同的语境上,我们就要通过神经网络的训练,给**相似语境下使用的相似的词语赋予相似的数字**

**本质是概率的预测**

![image-20230718190657019](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718190657019.png)

比如,现在有两句话,Troll2 is great 和  Gymkata is great,举个最简单的做法,通过**上个词语来预测下一个词语**

如果输入is,我希望预测出下一个单词great,    输入的词语为1,其余为0  ,放进神经网络中,然后随机初始化权重.这里的激活函数是y=x,不做非线性变化

==有几个激活函数,就代表每一个词语有几个特征(权重),上图有两个激活函数,代表每个词语有两个特征(权重),对应的权重的值就是激活函数之前的那些权重的值==

观察上图,通过计算,我们预测is的下一个单词是is,而不是great,(great的概率不是最大),所以,我们就利用损失函数,计算误差,反向传播,找出最优权重

![image-20230718191636582](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718191636582.png)

刚刚讲到,激活函数的数量对应每个词单词权重(特征)的数量,每个单词我们可以画在图上.换句话讲,可以表示成**向量的形式,一个词向量有几个维度,取决于有几个权重(特征),取决于激活函数**

Troll2和Gymkata都是电影的名字,也位于两个句子的第一个单词,所以语境和含义都相似,我们希望这**两个词向量能够相似一点**

通过训练神经网络,调整权重,达到下面效果

![image-20230718192239318](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718192239318.png)

Troll2和Gymkata两个词语很接近

我们将,用权重表示词语的这个过程,称为Word Embedding

但是,问题来了,通过一个单词预测下一个单词,好像不利于机器去'理解'语境,因为一个词语可能需要借助多个词语理解,所以衍生出两个典型的word2vec模型

### Skip Gram 和 CBOW

CBOW利用两边的词语来预测中间的词语,原理跟上面类似,输入的词语是1,其他为0

Skip Gram利用中间的词语来预测两边的词语

然而现实情况,网络远远比上面的要复杂的多,激活函数上百个,也就是一个词向量可以用上百个权重(特征),用更长的句子,更多的词语和训练集

![1689679840270](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689679840270.png)

### 减少运算量的方法

1. 因为输入的词语是1,其他为0,所以第一层输入层,为0的结点和权重可以忽略
2. 随机选择几个不想预测的输出,比如A想预测aardvark,不想预测abandon,所以输出层只计算aardvark和abandon

## encoder--decoder模型

解决从一个**序列**数据向另一个**序列**数据的转换的问题(seq2seq问题)

我们想要将Let's  go 从英语翻译到西班牙语言,两种语言有不同的单词,还有不同的句子长度

 ### encoder(编码器)

现在,我们有一个句子,Let's  go进行编码,用到前面的LSTM模型和word2vec

![image-20230718200103193](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718200103193.png)

我们不是把单词直接输入LSTM,而是经过word2vec转换的英语数据,<EOS>end of sentence,是个句号

这还不够,现实中,为了增加更多的权重和bias去拟合模型,我们会**增加多个LSTM单元**.前面讲过,一个词向量有多个权重(特征),**对于一个单词而言,一个LSTM单元处理一个权重.**词向量有几个权重,就有几个LSTM单元.

==注意:分出开的,处理另外词向量权重的单元,他们的权重和bias跟原来的不一样==



<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718200530450.png"/>

为了更进一步的增加参数去拟合模型,我们会**增加多个LSTM层**.

根据前面的内容,LSTM模型有为上方绿色的,负责传输长期记忆的线,下方紫色的,负责短期记忆的线.   **短期记忆的线的输出会作为增加的LSTM层的输入**![image-20230718224834337](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718224834337.png)

 

### decoder(解码器)

![image-20230718231822149](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718231822149.png)



从LSTM出来的长期记忆和短期记忆被称为**文本向量**

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718232125858.png"/>

解码器(decoder)和编码器(encoder)的LSTM的权重独立,两者不一样

第一件事就是将从encoder输出的文本向量连接到新的LSTM网络,这个新的LSTM网络跟encoder一样,有两层,每一层有两个单元

并且,**文本向量被用来初始化decoder(解码器)的LSTM网络,也就是作为长期记忆和短期记忆的输入**

编码器和解码器有着不同的权重和bias

解码器的最终目的是将向量,一堆数据转化成真的'句子'

![image-20230718233303432](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230718233303432.png)

然后跟编码器一样,我们输入经过word2vec转化后的**西班牙语数据**,但是我们先输入的是<EOS>.然后经过LSTM,有2层,每一层有两个单元

![image-20230719083541407](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719083541407.png)

解码器(decoder)**最上面的LSTM层的短期记忆的输出**,被送去全连接层,然后经过softmax函数转化为概率,对应的单词就是西班牙语言的vamos

但翻译过程还没有结束,知道解码器翻译出<EOS>才算是句子的翻译结束

![image-20230719084036731](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719084036731.png)

然后我们继续展开这个LSTM网络,**第二次的输入,是上一次的预测输出**的vamos,经过LSTM,在经过全连接层,最终输出了<EOS>,代表翻译结束.  我们成功将Let's go 翻译成了  vamos

### 训练

在上面的预测过程中,<EOS>的下一个输入是vamos,把上一个的输出作为下一个的输入

但在**训练**过程中,我们用**已知的,正确的单词作为下一层的输入**,比如解码器的第一个LSTM输入<EOS>,却输出了ir,是个错误的输出;所以我们下一个LSTM输人的应该是正确的,已知的vamos,而不是ir.

而且在**训练**过程中,不是一直翻译,直到解码器输出<EOS>,在**达到了正确的已知长度**后也会停止.比如let's go 翻译成 vamos.   一个单词加个结尾符号,长度为2,即使解码器第二个展开的LSTM输出的是ir,而不是<EOS>,也会视为翻译结束,因为已经达到了正确的句子的长度.然后调整参数,让其能够正确的预测

---

现实情况中,LSTM不只两层,而是有多层;每一层不只是两个单元,而是上千个单元,因为一个词向量有上千个权重,不同单元处理不同的权重.

对应的最终的解码器decoder 传输到 全连接层的输入,也有上千个输入(每个词向量的权重数量),对应上万个最终概率输出,因为是要输出一个单词,一种语言有上万的单词
