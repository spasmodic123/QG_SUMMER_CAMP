# 神经网络

## 何为神经网络的学习

一旦数据集中的所有数据点都通过了网络，我们就可以说一个epoch已经完成。

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689045689615.png"/>

一次训练有多个epochs,数据多次传递

## 随机梯度算法(SGD)

```python
# 建立神经网络模型
model = Sequential([
    Dense(units=512,input_shape=(28*28,),activation='relu'),
    Dense(units=10,activation='softmax')
])

model.compile(  # compile()传递了优化器(优化函数),损失函数,以及我们想要看到的指标
    optimizer=Adam(learning_rate=0.0001),  # Adam算法是一种随机梯度下降算法
    loss="categorical_crossentropy",
    metrics=['accuracy']
)

model.fit(
    x=x_train,
    y=y_train,
    batch_size=10,  # 一次向模型发送10个样本
    epochs=20,  # 全部数据经过模型20次
    shuffle=True,  # 数据洗牌,打乱数据
    verbose=2  # 2个日志记录
)



# 结果,看出随着训练的进行,损失函数值减少,精确率增加
Epoch 10/20
6000/6000 - 16s - loss: 0.0256 - accuracy: 0.9938 - 16s/epoch - 3ms/step
Epoch 11/20
6000/6000 - 17s - loss: 0.0216 - accuracy: 0.9948 - 17s/epoch - 3ms/step
Epoch 12/20
6000/6000 - 17s - loss: 0.0180 - accuracy: 0.9960 - 17s/epoch - 3ms/step
Epoch 13/20
6000/6000 - 16s - loss: 0.0148 - accuracy: 0.9971 - 16s/epoch - 3ms/step
Epoch 14/20
6000/6000 - 16s - loss: 0.0122 - accuracy: 0.9978 - 16s/epoch - 3ms/step
Epoch 15/20
6000/6000 - 17s - loss: 0.0101 - accuracy: 0.9983 - 17s/epoch - 3ms/step
Epoch 16/20
6000/6000 - 17s - loss: 0.0083 - accuracy: 0.9989 - 17s/epoch - 3ms/step
Epoch 17/20
6000/6000 - 17s - loss: 0.0070 - accuracy: 0.9990 - 17s/epoch - 3ms/step
Epoch 18/20
6000/6000 - 17s - loss: 0.0056 - accuracy: 0.9992 - 17s/epoch - 3ms/step
Epoch 19/20
6000/6000 - 17s - loss: 0.0045 - accuracy: 0.9996 - 17s/epoch - 3ms/step
Epoch 20/20
6000/6000 - 17s - loss: 0.0040 - accuracy: 0.9994 - 17s/epoch - 3ms/step
```

## 预测

```python
# 模型预测
prediction = model.predict(
    x=x_test,
    batch_size=10,
    verbose=1
)
for i in range(5):
    print(prediction[i])
    
    
#结果
# [1.5765100e-07 2.2723780e-10 1.5910067e-05 3.6559248e-04 3.9271711e-10
#  1.7562368e-07 9.5145170e-13 9.9961543e-01 4.0251831e-07 2.3608497e-06]
# [7.7392264e-07 6.8146001e-05 9.9970311e-01 1.6512492e-04 8.7406211e-14
#  1.8420375e-05 4.2345957e-05 3.2205594e-10 2.1628621e-06 1.7322250e-11]
# [8.2852063e-07 9.9483478e-01 2.7394507e-03 3.8868238e-04 1.0728442e-04
#  5.5759789e-05 7.0825648e-05 6.6760078e-04 1.1303802e-03 4.4597255e-06]
# [9.9997962e-01 2.1234193e-11 8.0121854e-06 9.0892179e-08 2.9640925e-12
#  2.4460746e-07 2.1883823e-06 7.7016803e-06 4.5301687e-09 2.1758046e-06]
# [1.8581100e-06 3.5931970e-07 4.7599631e-05 8.5706620e-07 9.8842013e-01
#  8.8175369e-07 8.3759605e-06 5.4352346e-05 3.4691609e-05 1.1430876e-02]

'''输出前五个预测数据,每一个预测输出都是一个列表,列表有10列,因为我们输出结点为10个,也就是结果可以分为10个类别
每一个预测输出列表中的10个值加起来=1,就是说预测本质上就是概率的预测,
每个预测输出的每个数值,代表最终预测结果是该类别的概率'''
```





## 常用损失函数(MSE)

均方误差

对于单个样本，利用MSE，我们首先计算所提供的输出预测与标签之间的差值（误差）。然后将误差平方。对于单个输入，我们只需这样做。如果我们一次向模型传递多个样本（一个batch），那么我们将取所有这些样本的平方误差的平均值。<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689058998687.png"/>

## 学习率

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689059773628.png"/>

学习率作为超参数,要经过多次测试寻找最优,一般原则是设置在0.01到0.0001之间

## 过拟合overfitting

过拟合的概念可归结为模型无法很好地泛化这一事实。它已经很好地学习了训练集的特征，但如果我们给模型提供的任何数据与训练时使用的确切数据略有偏差，它就无法泛化并准确预测输出。(可以理解为刷题太多,思维固化)

### 防止过拟合方法

1. 增大数据量(包括增加数量和多样性)

2. 适当减少模型复杂性(比如适当减少神经网络的层数)

3. 丢弃

   1. Dropout背后的一般想法是，如果您将其添加到模型中，它将在训练过程中随机忽略给定层中的某些节点子集，即从该层中剔除节点。因此，它被称为 "丢弃"（dropout）。这将防止这些被剔除的节点参与对数据的预测。

4. 数据扩增

   1. 裁剪 

   2. 旋转

   3.  翻转 

   4. 缩放

      数据增强的一般概念允许我们向训练集中添加更多的数据，这些数据与我们已有的数据相似，只是在某种程度上进行了**合理**的修改，使其不完全相同。

      例如，如果我们大部分的狗图像都是面朝左侧的狗，那么添加增强的翻转图像就是一种合理的修改，这样我们的训练集中也会有面朝右侧的狗。

## 欠拟合underfitting

欠拟合与过拟合相反。当一个模型甚至无法对其训练过的数据进行分类，更不用说它以前从未见过的数据时，该模型就被称为欠拟合。

### 防止欠拟合方法

1. 适当增加模型的复杂性

2. 增加特征

3. 减少丢弃(reduce dropout)

   1. 与减少过拟合的技巧正好相反。

      "丢弃"（dropout）是一种正则化技术，它随机忽略给定图层中的一个节点子集。从本质上讲，它阻止了这些被忽略的节点参与对数据的预测。

# 卷积神经网络CNN

CNN的隐藏层被称为卷积层,这些层是CNN的关键

卷积神经网络能够检测图像的模式

## 滤波器和卷积运算(Filters And Convolution Operations)

数学上,卷积操作被叫为---交叉相关

对于每一个卷积层,我们根据需要选择不同的滤波器,到底需要几个滤波器,这些滤波器是检测模式的

### 检测模式

试想一下，在任何一张图像中都可能会出现许多图案。多种边缘、形状、纹理、物体等。这些就是我们所说的模式。==滤波器就是一种模式检测器==

***边缘 形状 纹理 曲线 物体 颜色***

网络越深,滤波器越复杂,更深的层中,滤波器可以检测更加复杂的物体,比如动物

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/20230711205852.png"/>

### 滤波器(filters)(模式检测器)     (也是一个矩阵)

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689079799577.png"/>

有几个滤波器救有几个输出

==滤波器就是一种模式检测器==

### 卷积层(可视化)

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/5c7cb9e5.gif"/>

输入通道----蓝色   滤波器----9个格子,也就是3*3的矩阵(张量)

输出通道----绿色格子

对于蓝色输入通道的每个位置，3 x 3 滤波器都会进行计算，将蓝色输入通道的阴影部分映射到绿色输出通道的相应阴影部分。

### 卷积操作

卷积操作就是滤波器(filters)(一个矩阵)和像素块进行点积

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689081914184.png"/>

### 其他

在网络学习过程中出现模式检测器(滤波器)(filter)  ,       模式检测器是由网络自动生成的。滤波器的值一开始是随机的，随着网络在训练过程中的学习，滤波器的值会发生变化。过滤器的模式检测能力是自动产生的。



## 卷积网络可视化

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689081508398.png"/>

我们选择**上边缘滤波器(filter)**,可以看到,卷积之后,数字0的上边缘呈现红色(正数,正数越大,代表越正激活),下边缘呈现蓝色(负数,负数越负,代表越负激活)      可以看出,这个滤波器(filter)成功检测出**上边缘**这个**图像模式**

<img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689081784838.png"/>

我们换一张图片,是一个肩挎包,用**底部边缘检测器(filter)**,可以看出卷积之后,图像的基本形状大概保留,下边缘呈现红色(正数,正激活),上边缘呈现蓝色(负数,负激活),      可以看出,这个滤波器(filter)成功检测出图像的**下边缘**这个**图像模式**



同理,更加深层的卷积层,更加复杂的滤波器(filter)可以检测更加复杂的事物

# 机器学习

## onehot编码

**onehot编码是把类别变量转化为买机器学习算法易于利用的一种形式的过程**

第一步，先要给每个类别值都分配一个整数值。

比如，用 1 表示红色（red），2 表示绿色（green），3 表示蓝色（blue）

这种方式被称为标签编码或者整数编码，可以很轻松地将它还原回类别值.实际上，使用整数编码会让模型假设类别间存在自然的次序关系，从而导致结果不佳或得到意外的结果（预测值落在两个类别的中间）

在颜色（color）的示例中，有 3 种类别，因此需要 3 个二值变量进行编码。对应的颜色位置上将被标为“1”，其它颜色位置上会被标为“0”。

```python
red, green, blue
1, 0, 0
0, 1, 0
0, 0, 1
```



## 训练train,验证validation,测试test集合

需要区分的是训练集和验证集

训练集用于训练模型,每一个epoch通过网络后,会计算损失函数,更新权重,不断优化

验证集用于**验证模型是否过拟合或者欠拟合**,用于验证集验证时,**模型不会计算损失函数和更新权重**.在训练过程中，如果我们也在验证集上对模型进行验证，并发现验证数据的结果与训练数据的结果一样好，那么我们就可以更加确信我们的模型没有过度拟合。

测试集与其他两个集的一个主要区别是，测试集不应被标记。训练集和验证集必须有标签，这样我们才能看到训练过程中给出的指标，如每个epoch的损失和准确率。

**训练集就是日常作业,可以修改答案;验证集就是月考,但是答案不能修改,只能知道分数;测试集就是期末考试**



## 无监督学习Unsupervised learning

在**没有任何标签**的情况下，它将根据对数据结构的了解，学习如何创建从给定输入到特定输出的映射

### 无监督例子

1. 自动编码器

   1. 最基本的术语是，自动编码器是一种人工神经网络，它接收输入，然后输出对输入的重构。

      <img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689074520726.png"/>

2. 聚类算法

   1. 假设我们有一个特定年龄组的男性和女性的身高和体重数据,但是没有标签告诉程序是男的还是女的.我们可以想象，如果我们将身高和体重数据绘制在图表上，那么图表可能是这样的：体重在X轴上，身高在Y轴上。

   2. 虽然没有明确告诉我们这些数据的标签，但我们可以看到这里有两个非常明显的聚类，因此我们可以推断出这种聚类可能是基于这些个体是男性还是女性而发生的。

      其中一个聚类可能主要由女性组成，而另一个聚类则主要由男性组成，因此聚类是利用无监督学习的一个领域。

      <img src="https://spasmodic.oss-cn-hangzhou.aliyuncs.com/1689068574397.png"/>

## 半监督学习Semi_supervised learning

半监督学习是监督学习和无监督学习的结合

适用于数据量极为庞大的情况下

### 伪标记技术

1. 对部分数据集进行了人工标注。现在，我们将使用这些标注数据作为模型的训练集。然后，我们将训练我们的模型，就像训练其他标注数据集一样。**(监督学习)**通过*常规的训练过程，我们的模型表现相当不错*

2. 我们在数据集的标注部分训练完模型后，使用模型对剩余的未标注部分数据进行预测，然后将这些预测结果标注到每一条未标注数据上，并标注上预测结果的单独输出。(**无监督学习**)

3. 用神经网络预测的输出来标注未标注数据的过程就是伪标注的本质。**

4. 通过伪标注过程对未标注数据进行标注后，我们在完整数据集上训练我们的模型，该数据集现在由**真正标注的数据**和**伪标注的数据**组成。









