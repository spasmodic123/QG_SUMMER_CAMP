[TOC]

# Attention简易理解

## 为何要attention

对于简单的,短的文本,之前提到的encoder--decoder模型可以很好处理,但是一个句子很长,或者一个句子很复杂,即使是用来长期记忆的LSTM模型也会很难处理,这就需要引入attention.

attention的优点就是不受短时记忆的影响

还是以let's go 翻译成 西班牙语vamos. 作为例子

**attention就是给==解码器==的每一个步骤添加一个路径,这个路径可以直接访问==编码器==的的输出**,    ==一个解码器的输出可以对应多个编码器的输出==

给模型添加attention的方式并不唯一,这里介绍一种方式

## 具体过程

### 判断相似性

第一件事,是判断编码器的输出,和解码器的输入的相似性,通过余弦相似性.

之前提到过,一个单词可以用词向量表示,向量中的每一个元素就是该单词对于的权重.既然是向量,我们就可以利用两个向量之间的角度差值,计算相似性,具体方法如下

### 余弦相似性 

余弦相似性通过测量两个[向量](https://zh.wikipedia.org/wiki/向量)的夹角的[余弦](https://zh.wikipedia.org/wiki/余弦)值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关

![image-20230719094926106](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719094926106.png)

**实际中,我们只计算分子部分**



![image-20230719103159370](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719103159370.png)

在这里,第一步先计算英语的let's *(第一个编码器输出)*和 西班牙语<EOS>*(第一个解码器输出)*的相似性, 还计算go(第二个编码器输出) 和 <EOS>(第一个解码器输出)的相似性

![image-20230719103738061](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719103738061.png)

我们可以获得let's的词向量和<EOS>的词向量,计算,得到-0.41

![image-20230719103929063](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719103929063.png)

对于go 和 <EOS>一样道理,最终,我们算得,let's和<EOS>的相似性是-0.41,而go和<EOS>的相似性是0.01,后者大

**所以我们想要 go 能够对解码器的第一个输出产生更大的影响**

### softmax

先经过一个softmax函数,将相似性转换成概率分布

![image-20230719104551998](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719104551998.png)

然后,我们利用概率,去按比例缩放编码器的输出,

因为let's的编码器输出的向量是[-0.76,0.75],就把这个向量乘以0.4(概率), 

go的编码器输出的向量是[0.01.-0.01],乘以0.6(概率)

**然后相加,最终得到的值,[-0.76,0.75]x0.4+[0.01.-0.01]x0.6我们称为<EOS>的attention的值**==(可以理解为,根据重要性,进行加权求和)==,也就是[-0.3,0.3]  四舍五入之后

### 决定最终的输出词汇

将attention的值,和<EOS>的解码器的输入值,添加到全连接层

![image-20230719105504018](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719105504018.png)

attention是[-0.3,0.3],分别对应词向量第一个权重的attention,和第二个权重的attention,  0.9是<EOS>词向量的第一个权重,0.4是第二个权重

然后经过全连接层的计算,在经过softmax函数,得到最终的预测概率,概率最高的那个单词就是最终的预测单词.

观察最后一个全连接层,如果是不加attention的encoder--decoder模型,只输入0.9和0.4,也就是<EOS>的权重来计算最终概率,而加了attention之后,全连接层的输入维度从2变为了4

---

**以上的全部内容,就是给一个encoder--decoder模型添加attention模型的过程**





# Attention模型



具体计算过程,可以抽象成2个阶段

![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-b3da4bb2f08f7b2cd61c10353c502459_1440w.webp)

==**第一个过程是根据query和key计算权重系数矩阵,第二个过程根据上一阶段计算的权重系数矩阵对value进行加权和**==而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的**相似性或者相关性**；第二个阶段对第一阶段的原始分值进行**归一化**处理；这样，可以将Attention的计算过程抽象为如图展示的三个阶段。

![img](https://pic4.zhimg.com/80/v2-99c73a55cee546d47549cdfd0946adf7_1440w.webp)

在第一个阶段,计算相关性和相似性,常见的方法包括求向量之间的点积或者余弦相似性

然后归一化,记得除以一个$$\sqrt{dk}$$,**dk是词向量的维度**,防止方差过大,从而导致softmax几乎将全部的概率分配给了最大值对应的标签,导致某些维度的值太小,从而导致求梯度时值太小,出现梯度消失

![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-e5490b433a52c460154b497128a555b1_1440w.webp)

然后计算结果就是权重系数矩阵,然后和value加权求和即可得到attention的数值



# Self Attention

## 与attention的区别

self attention和attention的区别主要在不同任务中的使用方法.

Attention (AT) 经常被应用在从编码器（encoder）转换到解码器（decoder）,比如上面讲的,提供一个解码器和编码器之间的路径,用于两个组件之间

而self attention只应用于当前的一个组件,self attention比较擅长在一个序列当中，寻找不同部分之间的关系。比如说，在词法分析的过程中，能够帮助去理解不同词之间的关系。attention却更擅长寻找两个序列之间的关系，比如说在翻译任务当中，原始的文本和翻译后的文本

==self attention就是寻找序列中不同元素之间的相关关系==

## 具体解释

自注意力机制是注意力机制的变体，其减少了对外部信息的依赖，更擅长捕捉数据或特征的**内部相关性**

自注意力机制的计算过程：

1. 将输入单词转化成嵌入向量；

2. 根据嵌入向量得到q，k，v三个向量；

3. 为每个向量计算一个score：score =q . k ；

4. 为了梯度的稳定，Transformer使用了score归一化，即除以$$\sqrt{dk}$$；

5. 对score施以softmax激活函数；

6. softmax点乘value，得到加权的每个输入向量的评分v；

7. 相加之后得到最终的输出结果z ：z= ∑ v。

### 通俗理解query,key,value (KQV)

图书管（source）里有**很多书（value）**，为了方便查找，我们给书做了**编号（key）**。当我们想要了解**漫威（query）**的时候，我们就可以看看那些动漫、电影、甚至二战（美国队长）**相关**的书籍。为了提高效率，并不是所有的书都会仔细看，针对漫威来说，动漫，电影相关的会看的仔细一些**（权重高）**，但是二战的就只需要简单扫一下即可**（权重低**）。当我们全部看完后就对漫威有一个全面的了解了。



## self attention过程可视化

1. 第一步,将单词转化为嵌入向量,典型方法就是通过word2vec模型,转化为词向量

   a~1~,a~2~,a~3~,a~4~代表四个不同的单词,四个不同的词向量

2. 乘以对应的参数矩阵W^q^,W^k^,W^v^,得到q,k,v.      **(参数矩阵W的参数为可学习参数,经过训练形成)**

3. 不同单词向量的q和k可以**点乘,或者求余弦相关性**,得到两者之间的**相关性**

    ![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-8b5486f0069ad3aae1908820670acee3_1440w.webp)

   **a~1,1~代表a1与其自身的相关性,a~1,3~表示a~1~和a~3~之间的相关性**    ==a~i,j~代表的就是单词i和单词j之间的权重系数==

   

4. 然后除以一个$$\sqrt{dk}$$,dk是词向量的维度,再通过softmax函数归一化

5. 将权重系数**(q和k相乘的结果)**乘以v,然后输出b~1~,就是a~1~**考虑了所有其他单词的关系之后**,输出的向量

   ![image-20230719152105490](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719152105490.png)

   因为上图计算时考虑到都是a~1~与其他单词的关系,所以输出的是b1

   同理,输入考虑到是a~2~与其他单词的关系,输出的就是b~2~了



## self attention矩阵形式可视化

1. 计算各个单词对应的q,k,v

   ![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-32fa8145037247f4b14470227d4c0b75_1440w.webp)

2. 将不同的单词向量之间的k和q互相乘积,计算相关性,然后除以$$\sqrt{dk}$$,在用softmax函数归一化

   ![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-4cac9a5f49d7e5febf26c2c0bdd72be8_1440w.webp)

3. 最后乘以各自词向量的v,得到考虑了与其他的所有单词向量的相关性之后的最终输出

   ![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-96f832c590ea91d2913c2eec3f606285_1440w.webp)

---

完结咯,上面这种通过query和key的相似程度来确定value的权重分布的方法叫做**scaled dot-product attention**



# Multi head Self attention(多头注意力)

![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-f221c5a13a4e6e3fb84685e0f884b1da_1440w.webp)

我们上面提到了很多次相关性,但是现实情况中,相关性有很多种,语境非常复杂,所以不能只有一个k,一个q,一个v

==一个词向量有多个k,多个q,多个v==

比如有几个head,就把上面讲到的scaled dot-product attention重复几次,**每一次对应其中一种相关性的k,q,v**

## 如何计算

举个简单例子,只有两种相关性,kqv只有两种

计算的时候就是对应的同一类的kqv计算,得到2种不同种类的attention,然后将2个种类的attention拼接起来,再乘以一个参数矩阵W,得到对应单词的最终输出

![image-20230719160603339](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719160603339.png)

![img](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/v2-124e3b70dd715d48faacfc2a1a644160_1440w.webp)



# Positional Econding(位置编码)

在训练 self attention 的时候，实际上对于位置的**信息是缺失**的，没有前后的区别，上面讲的 a1,a2,a3 不代表输入的顺序，只是指输入的向量数量，不像 RNN，对于输入有明显的前后顺序。 **Self-attention** 是**同时输入，同时输出**。

然而,现实情况是,我们说话是有顺序的

为了解决 Attention 丢失的序列顺序信息，提出了 **Position Embedding**，也就是对于输入 单词向量 进行 Attention 计算之前，在 单词向量中加上位置信息，也就是说 最终的单词向量为： **X~final_embedding~ = Embedding + Positional Embedding**

![image-20230719170653934](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719170653934.png)



# layers normalization

跟batch normalization不同

batch normalization是对同一通道(维度),不同特征,不同样本进行标准化

layers normalization是对同一个特征,同一个样本,不同维度进行标准化

![image-20230719172022326](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719172022326.png)

m为均值,σ是标准差



# Transformer

就是一个encoder--decoder模型,在此模型上增加了许多改进

## encoder部分

![image-20230719172848797](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719172848797.png)



里面出现了之前讲到过多残差结构,positional embedding(位置编码),mutil-head attention(多头注意力),add(残差分支和原来主分支特征矩阵相加),norm(层标准化),feed-forward(两层全连接层)

## decoder部分

![image-20230719185707715](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719185707715.png)

  首先,和我们之前讲的那样,decoder的**上一层的输出,会作为下一层的输入**

### Masked self attention

我们先将self attention和masked self attention做个对比

我们知道,self attention是输出b~1~的时候,是考虑了全部a~1~,a~2~,a~3~,a~4~的,

但是我们思考一下decoder解码器的工作,他是一个一个的输入的,然后根据前一个的输出作为下一个的输入,像是一个时间序列.也就是说,他是根据过去的状全部态决定未来的状态,不能提前知道未来.用下面的图来解释就是:输出b~1~时只能考虑a~1~,输出b~2~时可以考虑a~1~和a~2~,输出b~3~时可以考虑a~1~,a~2~,a~3~,以此类推![image-20230719190928319](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719190928319.png)



### decoder具体运作

![image-20230719192300350](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719192300350.png)

右边的部分就是decoder,可以看出,基本组成跟左边的encoder差不多,只是多了圈起来的一块,叫做**cross attention**,并且通过这一块将encoder的输入传递给decoder



![image-20230719202132905](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719202132905.png)

以语音识别作为例子,将起始符号begin输入给decoder,经过masked self attention 转化为一个向量,然后乘上一个参数矩阵W^q^,得到q(query),        然后a~1~,a~2~,a~3~也乘以一个参数矩阵W^k^,得到k(key),然后k和q相乘,接着归一化,得到a~1~^'^    a~2~^'^     a~3~^'^  上面多了一点代表归一化后的值.

再将a~1~^'^    a~2~^'^     a~3~^'^   乘以对应的v, 然后加起来,得到输出向量,然后就把该输出向量丢到全连接层.一直到最后输出一个词汇,然后该输出词汇又作为下一个输入

**k跟v来自encoder,q来自decoder**

然后,重复以上过程,直到输出结尾字符



### decoder训练

![image-20230719203857392](https://spasmodic.oss-cn-hangzhou.aliyuncs.com/image-20230719203857392.png)

Teacher Forcing,就是在**训练**的过程中,decoder每一次输入的都是**正确的答案**,而不是像使用模型的时候,用上一次的输出作为下一次的输入

每次decoder的最终输出都是一个概率分布(也即是一个向量,向量的各个元素加起来等于1),而正确答案是经过one-hot编码的向量,我们经过cross entropy损失函数计算,目的是为了损失函数最小,从而不断调整参数

但是训练时候decoder 每次输入都是正确的词向量,而测试时输入的又是由机器自己产生的上一次的输出,这样的话测试的错误率可能较高,所以我们在训练的是时候会随机的给decoder输入几个错误答案,这种方法叫做Scheduled Sampleing(随机样本)
